{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run a Hail notebook in the background\n",
    "\n",
    "On the *All of US* Workbench, users are logged out after 30 minutes of inactivity. For long running Hail jobs, this means that users may not have all notebook cells populated even though the notebook continues to run to completion.\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "    <b>If you wish to capture all Hail notebook cell outputs</b>, use this notebook to run your long-running Hail notebook (or any other long-running notebook).<p>But also note that your analysis <a href=\"https://support.terra.bio/hc/en-us/articles/360029761352-Preventing-runaway-costs-with-notebook-auto-pause-#h_de5698f5-3c82-4763-aaaf-ea7df6a1869c\">the cluster will autopause after 24 hours</a>. To prevent your cluster from shutting down if your background Hail job takes longer than 24 hours, be sure to log in and start a notebook, any notebook, to reset the autopause timer.</p>\n",
    "</div>\n",
    "\n",
    "How to use this notebook:\n",
    "1. Copy this notebook to the workspace that contains the long-running notebook you wish to run in the background.\n",
    "1. Edit the filename in `NOTEBOOK_TO_RUN` to be the name of the notebook in this workspace you wish to run.\n",
    "1. Use menu `Cell -> Run All` to tell the kernel to run this notebook to completion.\n",
    "1. Close this tab and do other work. (note: when you close the tab, the outputs in *this* notebook will no longer be updated, and that is fine, because what we really want are the outputs in the Hail notebook.)\n",
    "1. When the Hail job is complete:\n",
    "    1. Your Hail cluster will pause (if you have no other notebooks open).\n",
    "    1. You will see a copy of your notebook in the notebooks tab with a date and timestamp suffix --> this is the output file.\n",
    "    1. You will also see the Hail log in `gs://<workspace bucket>/hail-logs/YYYYMMDD/hail*.log`\n",
    "\n",
    "If you check back, and you do not see the output notebook, the Hail job is still running. To confirm this, first notice whether your Cloud analysis environment is still running. If it is, from the terminal run `yarn application -list` to see that a spark context is still processing data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nbformat\n",
    "from nbconvert.preprocessors import CellExecutionError\n",
    "from nbconvert.preprocessors import ExecutePreprocessor\n",
    "\n",
    "import os\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "<b>Change the cell below</b> to hold the name of the notebook in this workspace that you wish to run in the background. There is no need to modify any of the other cells.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------[ CHANGE THIS TO BE THE NAME OF THE NOTEBOOK YOU WISH TO RUN ]-----------\n",
    "NOTEBOOK_TO_RUN = '02 Step 2 extract Nov 2021 SNPs from reference 1KGenomes and HGMD.ipynb'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\" >\n",
    "    NB: Change the following to <b>R</b> if you have an R notebook or to <b>PYTHON</b> if you have a python notebook.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "KERNEL = 'PYTHON'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Programmatically set output paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Formulate a new filename for the output notebook so that it include a date and timestamp for when it was executed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executed notebook will be written to filename \"02 Step 2 extract Nov 2021 SNPs from reference 1KGenomes and HGMD_20220329_220334.ipynb\" on the local disk and the workspace bucket.\n"
     ]
    }
   ],
   "source": [
    "TIMESTAMP_FILE_SUFFIX = time.strftime('_%Y%m%d_%H%M%S.ipynb')\n",
    "OUTPUT_NOTEBOOK = NOTEBOOK_TO_RUN.replace('.ipynb', TIMESTAMP_FILE_SUFFIX)\n",
    "\n",
    "print(f'Executed notebook will be written to filename \"{OUTPUT_NOTEBOOK}\" on the local disk and the workspace bucket.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\" >\n",
    "    NB: Run the next cell only if you use Hail\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hail logs will be copied to gs://fc-secure-30fdbdfd-a46b-406d-9617-1bc69ae1da9d/hail-logs/20220329/\n"
     ]
    }
   ],
   "source": [
    "DATESTAMP = time.strftime('%Y%m%d')\n",
    "HAIL_LOG_DIR_FOR_PROVENANCE = f'{os.getenv(\"WORKSPACE_BUCKET\")}/hail-logs/{DATESTAMP}/'\n",
    "\n",
    "print(f'Hail logs will be copied to {HAIL_LOG_DIR_FOR_PROVENANCE}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execute the notebook and capture provenance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_kernel(kernel):\n",
    "    return 'ir' if kernel.lower() == 'r' else 'python3'\n",
    "\n",
    "KERNEL_NAME = get_kernel(KERNEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "The next cell will use <kbd>nbconvert</kbd> to run the notebook in the background until it completes (or yields an error) and will capture all notebook outputs as a separate notebook file.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2022-03-29 22:03:42 WARN  Hail:43 - This Hail JAR was compiled for Spark 3.1.1, running with Spark 3.1.2.\n",
      "  Compatibility is not guaranteed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:===========================>                         (330 + 312) / 642]\r"
     ]
    }
   ],
   "source": [
    "# See also https://nbconvert.readthedocs.io/en/latest/execute_api.html\n",
    "with open(NOTEBOOK_TO_RUN) as f_in:\n",
    "    nb = nbformat.read(f_in, as_version=4)\n",
    "    ep = ExecutePreprocessor(timeout=-1, kernel_name=KERNEL_NAME)\n",
    "    try:\n",
    "        out = ep.preprocess(nb, {'metadata': {'path': ''}})\n",
    "    except CellExecutionError:\n",
    "        out = None\n",
    "        msg = 'Error executing the notebook \"%s\".\\n\\n' % NOTEBOOK_TO_RUN\n",
    "        msg += 'See notebook \"%s\" for the traceback.' % OUTPUT_NOTEBOOK\n",
    "        print(msg)\n",
    "    finally:\n",
    "        with open(OUTPUT_NOTEBOOK, mode='w', encoding='utf-8') as f_out:\n",
    "            nbformat.write(nb, f_out)\n",
    "\n",
    "# saving notebooks to the bucket\n",
    "WORKSPACE_BUCKET = os.getenv('WORKSPACE_BUCKET')\n",
    "!gsutil cp \"{OUTPUT_NOTEBOOK}\" {WORKSPACE_BUCKET}/notebooks/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\" >\n",
    "    NB: Run the next cell if you use Hail\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    When notebook has finished execution, the next cell will use <kbd>gsutil</kbd> to copy all hail logs from the execution directory on the local disk to the workspace bucket.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s $HAIL_LOG_DIR_FOR_PROVENANCE\n",
    "\n",
    "gzip --keep hail*.log\n",
    "gsutil -m cp hail*.log.gz ${1}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
